## ScriptArguments
model_name: "meta-llama/Llama-3.1-8B-Instruct"   ## 사용할 모델명 (huggingface model name)
dataset_path: "./data"                                ## 데이터셋 저장 경로

## SFTConfig
max_length: 16384                                     ## 모델이 수용 가능한 최대 시퀀스 길이. 너무 긴 시퀀스가 일부라면 이를 잘 조절하여 truncation할 수 있음.
output_dir: "./results/lr5e-6"                        ## 튜닝된 모델 저장 위치
report_to: "wandb"                                    ## 튜닝 로그 리포트 (토큰 등록 필요)
run_name: "lr5-6"
assistant_only_loss: true                             ## LM type dataset: 현재 손실 계산 시 LLM이 답변할 부분만 사용 (user, system messages 무시)
learning_rate: 5e-6                                   ## update matrix 학습률. 1e-4를 표준으로 감소시키며 확인
lr_scheduler_type: "cosine_with_restarts"             ## 코사인 스케줄러 사용 + 학습률 초기화
lr_scheduler_kwargs:
  num_cycles: 2                                       ## 코사인 곡선 사이클 횟수 지정
num_train_epochs: 80                                  ## 에폭
per_device_train_batch_size: 1                        ## GPU당 배치 사이즈
per_device_eval_batch_size: 1                         ## GPU당 배치 사이즈(평가)
gradient_accumulation_steps: 1                        ## 그래디언트를 모아두었다가 한꺼번에 적용: 배치 사이즈를 키우는 효과
do_eval: true                                         ## evaluation loss 계산 (전체 eval dataset을 대상으로 산출하므로, 꽤 많은 시간이 소요됨.)
eval_steps: 0.125
eval_strategy: "steps"                                 ## 일정 스탭마다 모델 평가
optim: "adamw_torch_fused"                            ## optimizer 설정
logging_steps: 0.0625
logging_strategy: "steps"                             ## 에폭별 로그 산출
save_strategy: "steps"                                ## 일정 스탭마다 모델 저장
save_steps: 0.125
weight_decay: 0.01                                    ## adamw optimizer에서 l2-norm weight decay. 과적합 방지.
max_grad_norm: 0.5                                    ## 그래디언트 클리핑의 임계값 지정. 모든 파라미터의 그래디언트에 대하여 l2-norm의 임계값. exploding 방지. 낮게 설정하면 훈련 속도 느려질 수 있음.
warmup_ratio: 0.03                                    ## 초기 학습률 warmup 단계의 비중 설정: 총 스텝 중 비율. 데이터셋과 모델 크기에 따라 조정. (모델와 데이터셋이 클수록 warmup_step을 키워주면 효과적)
bf16: true                                            ## bf16 정밀도 활성화: 모델 내부 연산 수행
tf32: true                                            ## nvidia TensorFormat-32 활성화: fp32로 수행되어야 하는 일부 연산을 가속
gradient_checkpointing: true                          ## 그래디언트를 캐시에 저장하지 않고 필요할 때마다 계산하여 GPU 절약
gradient_checkpointing_kwargs:
  use_reentrant: true
packing: true                                         ## 가능한 경우 여러 텍스트 시퀀스를 한 행에 병합하여 패딩으로 낭비되는 공간을 줄임. 데이터 시퀀스 길이 편차가 심할 때 유용. padding_free도 자동으로 활성화됨
dataloader_num_workers: 4                             ## 데이터로더 워커 수: 보통 GPU 개수 * 4 정도 사용한다고 함
push_to_hub: true                                     ## 허깅페이스에 모델 로드
dataset_kwargs:
  add_special_tokens: false                           ## 이미 chat_template에 special token 포함됨
  append_concat_token: false                          ## 이미 데이터셋의 각 항목들이 적절히 구분되어 있음

## LoraArguments
r: 64                                                 ## update matrix의 rank. 작을수록 많이 압축하여 품질 저하됨, 메모리 많이 할당됨
lora_alpha: 16                                        ## ∆Weight scaling factor. lora_alpha / r로 스케일링되며, 학습률 조정. 논문에서는 16을 고정하는 것을 추천했으나, 다른 값이 효과적일 때도 많다. r이 좀 커질 때 1/2수준으로 설정하면 적합
lora_dropout: 0.05                                    ## update matrics에서 dropout 적용 확률
bias: "none"                                          ## update matrix에 bias를 학습할 것인지 선택
task_type: "CAUSAL_LM"                                ## 튜닝 모형의 유형 지정
target_modules:                                       ## 적용할 트랜스포머 모듈. 기본(None)은 ["q_proj", "v_proj"]만 사용. embed_tokens와 lm_head는 제외
  - "q_proj"                                          ## Query
  - "k_proj"                                          ## Key
  - "v_proj"                                          ## Value
  - "o_proj"                                          ## Output
  - "up_proj"                                         ##
  - "down_proj"                                       ##
  - "gate_proj"                                       ##