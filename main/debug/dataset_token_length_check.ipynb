{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43107dd4",
   "metadata": {},
   "source": [
    "## 데이터셋의 토큰 길이를 체크하기 위한 노트북"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e614bc",
   "metadata": {},
   "source": [
    "### **1. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a43e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  ## utils 함수 이용을 위한 패스 설정\n",
    "\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from utils import remove_hangul\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt ## 히스토그램 뿌리기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8081d22e",
   "metadata": {},
   "source": [
    "### **2. 데이터셋 로드**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7fb3bc",
   "metadata": {},
   "source": [
    "`-` `csv` 포맷의 데이터셋과 시스템 프롬프트를 로드한 다음, 한글 및 불필요한 컬럼을 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeb83ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145862/145862 [00:45<00:00, 3202.53 examples/s]\n",
      "Map: 100%|██████████| 145862/145862 [01:14<00:00, 1954.03 examples/s]\n",
      "Map: 100%|██████████| 145862/145862 [00:12<00:00, 11637.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "## 유저 프롬프트\n",
    "df = pd.read_csv(\"../data/data_all.csv\", encoding = \"utf-8\")\n",
    "\n",
    "## 시스템 프롬프트\n",
    "with open(\"../data/system_prompt.txt\", \"r\") as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "## datasets.Dataset 객체로 변환\n",
    "ds = datasets.Dataset.from_pandas(df)\n",
    "columns_to_remove = [f for f in list(ds.features) if f not in [\"subject_id\", \"text\"]]\n",
    "\n",
    "## Explicit format\n",
    "train_ds = ds.map(\n",
    "    lambda sample: {\n",
    "        \"text\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": sample[\"text\"]}\n",
    "            ]\n",
    "    }\n",
    ")\n",
    "\n",
    "## 한글 제거 및 불필요한 컬럼 제거\n",
    "train_ds = train_ds.map(lambda sample: remove_hangul(sample, column = \"text\"))\n",
    "train_ds = train_ds.map(remove_columns = columns_to_remove, batched = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eddd7a",
   "metadata": {},
   "source": [
    "### **3. 토크나이저 로드 및 토크나이징**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258929db",
   "metadata": {},
   "source": [
    "`-` `Llama-3.1-8B-Instruct` 모델의 토크나이저를 로드한 뒤, 토큰화를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    use_fast = True,\n",
    "    trust_remote_code = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"{{ bos_token }}\"\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + eos_token }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n'}}\"\n",
    "            \"{% generation %}\"\n",
    "            \"{{ message['content'] +  eos_token }}\"\n",
    "            \"{% endgeneration %}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{%- if add_generation_prompt %}\"\n",
    "    \"{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"\n",
    "    \"{%- endif %}\"\n",
    ")\n",
    "\n",
    "tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n",
    "\n",
    "def template_dataset(example):\n",
    "    \"\"\"\n",
    "    데이터셋 객체에 매핑시키는 함수. \n",
    "    \"\"\"\n",
    "    return {\"token_size\": len(tokenizer.apply_chat_template(example[\"text\"], tokenize = True))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981bc43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145862/145862 [10:38<00:00, 228.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token_size: 17176\n",
      "min token_size: 230\n"
     ]
    }
   ],
   "source": [
    "train_ds = datasets.load_dataset(\"json\", \"../data/data_all.json\", split = \"train\")\n",
    "train_ds = train_ds.map(template_dataset)\n",
    "\n",
    "print(f\"max token_size: {max(train_ds['token_size'])}\")\n",
    "print(f\"min token_size: {min(train_ds['token_size'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9c9cb",
   "metadata": {},
   "source": [
    "### **4. 훈련 데이터셋의 토큰 길이 파악**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a48083d",
   "metadata": {},
   "source": [
    "`-` Truncation을 위해서 훈련 데이터셋만의 토큰 길이도 파악할 필요가 있습니다.\n",
    "\n",
    "* 입력 텍스트의 토큰 길이가 길어질수록 VRAM 사용량 또한 산술적으로 증가하므로, 이를 적당히 조율합니다.\n",
    "* 분포를 파악해서 큰 영향이 없도록 최대 토큰 사이즈를 조정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39562d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 90 examples [00:00, 4195.28 examples/s]\n",
      "Generating train split: 1781 examples [00:00, 11776.92 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 139.97 examples/s]\n",
      "Map: 100%|██████████| 1781/1781 [00:11<00:00, 149.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "sft_ds = datasets.load_dataset(\"json\", data_files = \"../data/sft_train_dataset.json\", split = \"train\")\n",
    "dpo_ds = datasets.load_dataset(\"json\", data_files = \"../data/dpo_train_dataset.json\", split = \"train\")\n",
    "\n",
    "sft_ds = sft_ds.map(\n",
    "    lambda example:\n",
    "        {\"token_size\": len(tokenizer.apply_chat_template(example[\"messages\"][:2], tokenize = True))}\n",
    ")\n",
    "dpo_ds = dpo_ds.map(\n",
    "    lambda example:\n",
    "        {\"token_size\": len(tokenizer.apply_chat_template(example[\"prompt\"], tokenize = True))}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dacde89",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Column' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtoken_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.to_csv(\u001b[33m\"\u001b[39m\u001b[33mall_token_length.csv\u001b[39m\u001b[33m\"\u001b[39m, encoding = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LLM/lib/python3.12/site-packages/pandas/core/frame.py:871\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    863\u001b[39m         mgr = arrays_to_mgr(\n\u001b[32m    864\u001b[39m             arrays,\n\u001b[32m    865\u001b[39m             columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    868\u001b[39m             typ=manager,\n\u001b[32m    869\u001b[39m         )\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m         mgr = \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    880\u001b[39m     mgr = dict_to_mgr(\n\u001b[32m    881\u001b[39m         {},\n\u001b[32m    882\u001b[39m         index,\n\u001b[32m   (...)\u001b[39m\u001b[32m    885\u001b[39m         typ=manager,\n\u001b[32m    886\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LLM/lib/python3.12/site-packages/pandas/core/internals/construction.py:319\u001b[39m, in \u001b[36mndarray_to_mgr\u001b[39m\u001b[34m(values, index, columns, dtype, copy, typ)\u001b[39m\n\u001b[32m    314\u001b[39m     values = _ensure_2d(values)\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    317\u001b[39m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[32m    318\u001b[39m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     values = \u001b[43m_prep_ndarraylike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy_on_sanitize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m values.dtype != dtype:\n\u001b[32m    322\u001b[39m     \u001b[38;5;66;03m# GH#40110 see similar check inside sanitize_array\u001b[39;00m\n\u001b[32m    323\u001b[39m     values = sanitize_array(\n\u001b[32m    324\u001b[39m         values,\n\u001b[32m    325\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    328\u001b[39m         allow_2d=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    329\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LLM/lib/python3.12/site-packages/pandas/core/internals/construction.py:580\u001b[39m, in \u001b[36m_prep_ndarraylike\u001b[39m\u001b[34m(values, copy)\u001b[39m\n\u001b[32m    578\u001b[39m     values = np.array([convert(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m580\u001b[39m     values = \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _ensure_2d(values)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LLM/lib/python3.12/site-packages/pandas/core/internals/construction.py:564\u001b[39m, in \u001b[36m_prep_ndarraylike.<locals>.convert\u001b[39m\u001b[34m(v)\u001b[39m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m v\n\u001b[32m    563\u001b[39m v = extract_array(v, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m res = \u001b[43mmaybe_convert_platform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We don't do maybe_infer_to_datetimelike here bc we will end up doing\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m#  it column-by-column in ndarray_to_mgr\u001b[39;00m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LLM/lib/python3.12/site-packages/pandas/core/dtypes/cast.py:136\u001b[39m, in \u001b[36mmaybe_convert_platform\u001b[39m\u001b[34m(values)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# The caller is responsible for ensuring that we have np.ndarray\u001b[39;00m\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m#  or ExtensionArray here.\u001b[39;00m\n\u001b[32m    134\u001b[39m     arr = values\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m == _dtype_obj:\n\u001b[32m    137\u001b[39m     arr = cast(np.ndarray, arr)\n\u001b[32m    138\u001b[39m     arr = lib.maybe_convert_objects(arr)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Column' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(train_ds[\"token_size\"]).to_csv(\"all_token_length.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f254e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
