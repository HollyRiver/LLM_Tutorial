{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43107dd4",
   "metadata": {},
   "source": [
    "## 데이터셋의 토큰 길이를 체크하기 위한 노트북"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e614bc",
   "metadata": {},
   "source": [
    "### **1. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25a43e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/LLM/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  ## utils 함수 이용을 위한 패스 설정\n",
    "\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from utils import remove_hangul\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt ## 히스토그램 뿌리기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8081d22e",
   "metadata": {},
   "source": [
    "### **2. 데이터셋 로드**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7fb3bc",
   "metadata": {},
   "source": [
    "`-` 데이터셋과 시스템 프롬프트를 로드한 다음, 한글 및 불필요한 컬럼을 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ecc2e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"argilla/ultrafeedback-binarized-preferences-cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b7888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'prompt', 'chosen', 'chosen-rating', 'chosen-model', 'rejected', 'rejected-rating', 'rejected-model'],\n",
       "        num_rows: 60917\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.remove_columns([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeb83ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/data_all.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## 유저 프롬프트\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/data_all.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m## 시스템 프롬프트\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m../data/system_prompt.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/data_all.csv'"
     ]
    }
   ],
   "source": [
    "## 유저 프롬프트\n",
    "df = pd.read_csv(\"data/data_all.csv\", encoding = \"utf-8\")\n",
    "\n",
    "## 시스템 프롬프트\n",
    "# with open(\"../data/system_prompt.txt\", \"r\") as f:\n",
    "#     system_prompt = f.read()\n",
    "\n",
    "## datasets.Dataset 객체로 변환\n",
    "ds = datasets.Dataset.from_pandas(df)\n",
    "columns_to_remove = [f for f in list(ds.features) if f not in [\"subject_id\", \"text\"]]\n",
    "\n",
    "## Explicit format\n",
    "train_ds = ds.map(\n",
    "    lambda sample: {\n",
    "        \"text\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": sample[\"text\"]}\n",
    "            ]\n",
    "    }\n",
    ")\n",
    "\n",
    "## 한글 제거 및 불필요한 컬럼 제거\n",
    "train_ds = train_ds.map(lambda sample: remove_hangul(sample, column = \"text\"))\n",
    "train_ds = train_ds.map(remove_columns = columns_to_remove, batched = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eddd7a",
   "metadata": {},
   "source": [
    "### **3. 토크나이저 로드 및 토크나이징**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258929db",
   "metadata": {},
   "source": [
    "`-` `Llama-3.1-8B-Instruct` 모델의 토크나이저를 로드한 뒤, 토큰화를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    use_fast = True,\n",
    "    trust_remote_code = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"{{ bos_token }}\"\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + eos_token }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n'}}\"\n",
    "            \"{% generation %}\"\n",
    "            \"{{ message['content'] +  eos_token }}\"\n",
    "            \"{% endgeneration %}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{%- if add_generation_prompt %}\"\n",
    "    \"{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"\n",
    "    \"{%- endif %}\"\n",
    ")\n",
    "\n",
    "tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n",
    "\n",
    "def template_dataset(example):\n",
    "    \"\"\"\n",
    "    데이터셋 객체에 매핑시키는 함수. \n",
    "    \"\"\"\n",
    "    return {\"token_size\": len(tokenizer.apply_chat_template(example[\"text\"], tokenize = True))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61559db",
   "metadata": {},
   "source": [
    "`-` 전체 데이터셋의 토큰 길이 파악\n",
    "\n",
    "* 훈련 및 생성 작업에 참고할 수 있도록 토큰 길이를 파악합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981bc43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145862/145862 [10:38<00:00, 228.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token_size: 17176\n",
      "min token_size: 230\n"
     ]
    }
   ],
   "source": [
    "train_ds = datasets.load_dataset(\"json\", \"../data/data_all.json\", split = \"train\")\n",
    "train_ds = train_ds.map(template_dataset)\n",
    "\n",
    "print(f\"max token_size: {max(train_ds['token_size'])}\")\n",
    "print(f\"min token_size: {min(train_ds['token_size'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9c9cb",
   "metadata": {},
   "source": [
    "### **4. 훈련 데이터셋의 토큰 길이 파악**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a48083d",
   "metadata": {},
   "source": [
    "`-` Truncation을 위해서 훈련 데이터셋만의 토큰 길이도 파악할 필요가 있습니다.\n",
    "\n",
    "* 입력 텍스트의 토큰 길이가 길어질수록 VRAM 사용량 또한 산술적으로 증가하므로, 이를 적당히 조율합니다.\n",
    "* 분포를 파악해서 큰 영향이 없도록 최대 토큰 사이즈를 조정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39562d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 90 examples [00:00, 4195.28 examples/s]\n",
      "Generating train split: 1781 examples [00:00, 11776.92 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 139.97 examples/s]\n",
      "Map: 100%|██████████| 1781/1781 [00:11<00:00, 149.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "sft_ds = datasets.load_dataset(\"json\", data_files = \"../data/sft_train_dataset.json\", split = \"train\")\n",
    "dpo_ds = datasets.load_dataset(\"json\", data_files = \"../data/dpo_train_dataset.json\", split = \"train\")\n",
    "\n",
    "sft_ds = sft_ds.map(\n",
    "    lambda example:\n",
    "        {\"token_size\": len(tokenizer.apply_chat_template(example[\"messages\"][:2], tokenize = True))}\n",
    ")\n",
    "dpo_ds = dpo_ds.map(\n",
    "    lambda example:\n",
    "        {\"token_size\": len(tokenizer.apply_chat_template(example[\"prompt\"], tokenize = True))}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dacde89",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Column' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtoken_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.to_csv(\u001b[33m\"\u001b[39m\u001b[33mall_token_length.csv\u001b[39m\u001b[33m\"\u001b[39m, encoding = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LLM/lib/python3.12/site-packages/pandas/core/frame.py:871\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    863\u001b[39m         mgr = arrays_to_mgr(\n\u001b[32m    864\u001b[39m             arrays,\n\u001b[32m    865\u001b[39m             columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    868\u001b[39m             typ=manager,\n\u001b[32m    869\u001b[39m         )\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m         mgr = \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    880\u001b[39m     mgr = dict_to_mgr(\n\u001b[32m    881\u001b[39m         {},\n\u001b[32m    882\u001b[39m         index,\n\u001b[32m   (...)\u001b[39m\u001b[32m    885\u001b[39m         typ=manager,\n\u001b[32m    886\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LLM/lib/python3.12/site-packages/pandas/core/internals/construction.py:319\u001b[39m, in \u001b[36mndarray_to_mgr\u001b[39m\u001b[34m(values, index, columns, dtype, copy, typ)\u001b[39m\n\u001b[32m    314\u001b[39m     values = _ensure_2d(values)\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    317\u001b[39m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[32m    318\u001b[39m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     values = \u001b[43m_prep_ndarraylike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy_on_sanitize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m values.dtype != dtype:\n\u001b[32m    322\u001b[39m     \u001b[38;5;66;03m# GH#40110 see similar check inside sanitize_array\u001b[39;00m\n\u001b[32m    323\u001b[39m     values = sanitize_array(\n\u001b[32m    324\u001b[39m         values,\n\u001b[32m    325\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    328\u001b[39m         allow_2d=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    329\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LLM/lib/python3.12/site-packages/pandas/core/internals/construction.py:580\u001b[39m, in \u001b[36m_prep_ndarraylike\u001b[39m\u001b[34m(values, copy)\u001b[39m\n\u001b[32m    578\u001b[39m     values = np.array([convert(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m580\u001b[39m     values = \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _ensure_2d(values)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LLM/lib/python3.12/site-packages/pandas/core/internals/construction.py:564\u001b[39m, in \u001b[36m_prep_ndarraylike.<locals>.convert\u001b[39m\u001b[34m(v)\u001b[39m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m v\n\u001b[32m    563\u001b[39m v = extract_array(v, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m res = \u001b[43mmaybe_convert_platform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We don't do maybe_infer_to_datetimelike here bc we will end up doing\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m#  it column-by-column in ndarray_to_mgr\u001b[39;00m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LLM/lib/python3.12/site-packages/pandas/core/dtypes/cast.py:136\u001b[39m, in \u001b[36mmaybe_convert_platform\u001b[39m\u001b[34m(values)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# The caller is responsible for ensuring that we have np.ndarray\u001b[39;00m\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m#  or ExtensionArray here.\u001b[39;00m\n\u001b[32m    134\u001b[39m     arr = values\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m == _dtype_obj:\n\u001b[32m    137\u001b[39m     arr = cast(np.ndarray, arr)\n\u001b[32m    138\u001b[39m     arr = lib.maybe_convert_objects(arr)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Column' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(train_ds[\"token_size\"]).to_csv(\"all_token_length.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f254e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
