{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dadfc33",
   "metadata": {},
   "source": [
    "# HuggingFace LLM과 공개 데이터셋을 활용한 RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd56239",
   "metadata": {},
   "source": [
    "## **1. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ============= Basic Modules =============\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from dataclasses import dataclass, field, fields    ## For TrlParser\n",
    "\n",
    "## ================ Utility ================\n",
    "import os\n",
    "import argparse\n",
    "import wandb        ## External Training Log Analysis Tool\n",
    "\n",
    "## ========== Hugging Face Library ==========\n",
    "from huggingface_hub import login   ## Input Token\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import (\n",
    "    SFTTrainer, SFTConfig,  ## SFT\n",
    "    DPOConfig, DPOTrainer,  ## DPO\n",
    "    TrlParser               ## YAML Parameter Parsing\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer,    ## Vanilla Model Loading\n",
    "    HfArgumentParser, TrainingArguments,    ## Tuning Parameter Setting\n",
    "    BitsAndBytesConfig,                     ## Quantization\n",
    "    set_seed                                ## Seeding (not recommended)\n",
    ")\n",
    "\n",
    "from peft import LoraConfig ## For Low-Rank Adaption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78763284",
   "metadata": {},
   "source": [
    "## **2. UltraFeedback Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d02ed",
   "metadata": {},
   "source": [
    "* 고품질 AI 피드백 데이터셋. PPO에 활용되는 데이터셋을 이진 선호도 데이터셋으로 변환한 형태(RLAIF)\n",
    "* 특정 태스크에 목적이 있는 게 아닌, 다양한 주제에서의 채팅 기능 향상을 위한 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e42bc28",
   "metadata": {},
   "source": [
    "`-` 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd83df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 원시 데이터 로드\n",
    "ds = load_dataset(\"argilla/ultrafeedback-binarized-preferences-cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e6a414",
   "metadata": {},
   "source": [
    "`-` SFT / DPO 데이터셋 분리\n",
    "\n",
    "* 본래는 SFT를 위한 데이터셋과 DPO를 위한 데이터셋이 따로 준비되어 있어야 하지만, DPO 데이터셋만 존재하기 때문에 실습을 위해 둘을 분리하여 훈련을 진행합니다.\n",
    "* 실제 추천 프레임워크\n",
    "\n",
    "    1. 원하는 태스크의 라벨링된 데이터셋으로 SFT 수행\n",
    "    2. SFT를 완수한 모델을 이용하여 한 질문에 대하여 두 개 이상의 답변 생성\n",
    "    3. 생성된 답변을 조합하여 Chosen / Rejected 쌍으로 구성된 선호도 데이터셋 제작(HF or AIF)\n",
    "    > 해당 과정에서 가장 좋은 답변만을 Chosen으로 설정할 수 있음\n",
    "    >\n",
    "    > n개 답변을 생성했다면, 그중 한 개의 답변을 최선으로 설정한 뒤, 이를 Chosen으로 설정. 총 n-1개의 선호도 쌍을 생성 가능\n",
    "    >\n",
    "    > 일반적으로 순위를 매긴 뒤, 상위 몇 개의 답변만 Chosen으로 설정한 다음 조합하여 선호도 쌍을 생성\n",
    "    4. 선호도 데이터셋을 사용하여 SFT를 수행한 모델에 DPO를 이어서 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok = True)    ## 디렉토리 생성\n",
    "\n",
    "ds_split = ds[\"train\"].train_test_split(test_size = 0.5, seed = 42) ## SFT/DPO 데이터 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d7dc2",
   "metadata": {},
   "source": [
    "`-` 데이터셋 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For SFT\n",
    "sft_ds = ds_split[\"train\"]\n",
    "sft_ds = sft_ds.rename_column(\"chosen\", \"messages\").remove_columns([col for col in sft_ds.column_names if col != \"chosen\"]).train_test_split(test_size = 0.1, seed = 42)\n",
    "sft_ds[\"train\"].to_json(\"./data/sft_train_dataset.json\", orient = \"records\")\n",
    "sft_ds[\"test\"].to_json(\"./data/sft_test_dataset.json\", orient = \"records\")\n",
    "\n",
    "## For DPO: Implicit Prompt -> Explicit Prompt (Recommanded)\n",
    "dpo_ds = ds_split[\"test\"].map(\n",
    "    lambda sample: {\n",
    "        \"prompt\": [{\"role\": \"user\", \"content\": sample[\"prompt\"]}],\n",
    "        \"chosen\": [content for content in sample[\"chosen\"] if content[\"role\"] == \"assistant\"],\n",
    "        \"rejected\": [content for content in sample[\"rejected\"] if content[\"role\"] == \"assistant\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "dpo_ds = dpo_ds.remove_columns([col for col in dpo_ds.column_names if col not in [\"prompt\", \"chosen\", \"rejected\"]]).train_test_split(test_size = 0.1, seed = 42)\n",
    "dpo_ds[\"train\"].to_json(\"./data/dpo_train_dataset.json\", orient = \"records\")\n",
    "dpo_ds[\"test\"].to_json(\"./data/dpo_test_dataset.json\", orient = \"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8baa6d",
   "metadata": {},
   "source": [
    "## **3. Model Loading**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d4f4f8",
   "metadata": {},
   "source": [
    "* 허깅페이스에 로그인하여 권한을 획득하고, [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) 모델을 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22adab4",
   "metadata": {},
   "source": [
    "`-` 허깅페이스에 로그인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4153ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token = \"hf_...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a042bec7",
   "metadata": {},
   "source": [
    "> 해당 방식은 시연을 위한 방법이고, `login()`으로 작성하여 `token` 파라미터를 사용하지 않은 상태에서 터미널에서 토큰을 입력해 로그인하는 것이 안전합니다.\n",
    ">\n",
    "> 일반적으로는 터미널에서 `hf auth login` 명령어를 통해 미리 로그인해주는 것이 편합니다. 이러면 해당 로그인 코드는 필요하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a0d9a",
   "metadata": {},
   "source": [
    "`-` 모델 및 토크나이저 로드\n",
    "\n",
    "* 거의 대부분의 모델은 두 개의 객체를 함께 로드합니다.\n",
    "* 토크나이저(Tokenizer): 텍스트를 단어(실제론 이보다 작은 단위)로 쪼갠 다음(Tokenize), Token ID로 매핑\n",
    "* 모델(Model): 임베딩 후 생성\n",
    "\n",
    "    > 양자화 설정을 여기서 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f884a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    use_fast = True,            ## Rust로 구현된 Fast Tokenizer 사용 (Qwen, RoPE, ChatGLM 등의 특이한 구조에서는 호환 안됨)\n",
    "    trust_remote_code = True    ## 모델 코드 전체 다운로드 후 사용\n",
    ")\n",
    "\n",
    "## 양자화 설정: 모델의 가중치를 로드할 때 양자화하여 들여옵니다.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,                    ## 4비트로 양자화\n",
    "    bnb_4bit_use_double_quant = True,       ## 추가 양자화(스케일 파라미터 양자화) 활성화. 메모리 절약\n",
    "    bnb_4bit_quant_type = \"nf4\",            ## 양자화 데이터 타입 지정: 4비트 기반 모델 훈련 시 사용\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 ## 4비트로 로드하지만, attention 연산 시 해당 포맷으로 역양자화하여 처리 (라마 기본 자료형, 거의 대부분의 최신 LLM은 해당 포맷 사용)\n",
    ")\n",
    "\n",
    "## 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",         ## 사용 모델명\n",
    "    device_map = \"cuda:0\",                      ## GPU 사용\n",
    "    use_cache = False,                          ## KV 캐시 미사용(VRAM), 추론 속도 저하. gradienc_checkpointing과 동시 사용 불가\n",
    "    attn_implementation = \"flash_attention_2\",  ## flash_attention 연산 사용. sdpa가 더 빠르고 효율적일 수도 있음.\n",
    "    dtype = torch.bfloat16,                     ## 초기 가중치 로드 데이터 타입. Llama-3.1-8B의 자료형으로 설정\n",
    "    quantization_config = bnb_config            ## 양자화 설정 적용\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50f051",
   "metadata": {},
   "source": [
    "`-` 모델 및 토크나이저 설정\n",
    "\n",
    "* 토크나이저를 일부 커스터마이징합니다. Chat Template 설정이 주요 요소입니다.\n",
    "* Chat Template는 기본적인 템플릿이 따로 존재하지만, SFT에서 `assistant_only_loss`로 학습을 위해서는 추가적인 양식이 요구됩니다.\n",
    "\n",
    "    > `prompt-completion` 포맷의 데이터셋은 `completion_only_loss`를 활성화하는 것만으로도 간단하게 라벨 부분만으로 손실을 계산할 수 있습니다.\n",
    "    >\n",
    "    > 일부 모델의 토크나이저에는 `generation` 부분이 템플릿에 포함되어 있어 그대로 사용해도 될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa5b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token       ## 패딩할 토큰 설정 (padding_free 설정 시 큰 의미는 없음)\n",
    "tokenizer.padding_side = \"left\"                 ## 디코더이므로 왼쪽을 패딩 (마지막 토큰을 보고 생성)\n",
    "\n",
    "## 데이터셋에 적합한 chat template 적용: {% generation %} 부분을 추가하여 assistant_only_loss 진행\n",
    "## 모든 텍스트로 손실을 계산하고자 한다면 tokenizer에 기본으로 할당된 chat template로 충분\n",
    "## jinja2 template engine 구문. 파이썬 문법과 거의 동일\n",
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"{{ bos_token }}\"\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + eos_token }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n'}}\"\n",
    "            \"{% generation %}\"\n",
    "            \"{{ message['content'] +  eos_token }}\"\n",
    "            \"{% endgeneration %}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{%- if add_generation_prompt %}\"\n",
    "    \"{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"\n",
    "    \"{%- endif %}\"\n",
    ")\n",
    "\n",
    "## 새로운 Chat Template 적용\n",
    "tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81491b53",
   "metadata": {},
   "source": [
    "> 핵심은 `'assistant'`부분에 `{% generation %} ~ {% endgeneration %}`을 추가하는 것입니다. 이러면 해당 텍스트는 마스킹에서 제외됩니다.\n",
    ">\n",
    "> ```Python\n",
    "> \"{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n'}}\"\n",
    "> \"{% generation %}\"\n",
    "> \"{{ message['content'] +  eos_token }}\"\n",
    "> \"{% endgeneration %}\"\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4981bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 템플릿 적용사항 확인\n",
    "print(\"======== Log a few random samples from the processed training set ========\")\n",
    "for index in random.sample(range(len(train_ds)), 2):\n",
    "    print(tokenizer.apply_chat_template(train_ds[index][\"messages\"], tokenize = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71cd13",
   "metadata": {},
   "source": [
    "## **4. Supervised Fine Tuning (SFT)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ec36f",
   "metadata": {},
   "source": [
    "* 기본 모델을 원하는 태스크에 특화되도록 1차적인 파인튜닝을 시행합니다.\n",
    "* `YAML` 파일로 하이퍼파라미터를 쉽게 관리할 수 있습니다.\n",
    "\n",
    "    > 단순 `argparse` 모듈로 파라미터 설정을 할 수도 있지만, 이 경우 `accelerate` 라이브러리를 통한 분산 GPU 환경에서의 학습에 호환되지 않으므로 확장성을 위해 이와 같은 방식을 택했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aac8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zombie Process 발생 방지\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"    ## 수동 업데이트: wandb sync --include-offline ./wandb/latest-run\n",
    "wandb.init(project = \"RLHF\")\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "## TrlParser에 들어갈 class들을 커스터마이징: 하이퍼파라미터 저장\n",
    "@dataclass  ## 데이터 보관 클래스를 간단하게 구축 가능: __init__, __repr__, __eq()__등의 메소드 자동 생성\n",
    "class ScriptArguments:\n",
    "    dataset_path: str = field(default = None, metadata = {\"help\": \"dataset directory\"})\n",
    "    model_name: str = field(default = None, metadata = {\"help\": \"사용할 모델 ID\"})\n",
    "\n",
    "@dataclass\n",
    "class LoraArguments:\n",
    "    r: int = field(default = 64, metadata = {\"help\": \"update matrix의 rank. 작을수록 많이 압축하여 품질 저하됨, 메모리 많이 할당됨\"})\n",
    "    lora_alpha: int = field(default = 32, metadata = {\"help\": \"∆Weight scaling factor. lora_alpha / r로 스케일링되며, 학습률 조정. 보통 1/2 수준으로 설정\"})\n",
    "    lora_dropout: float = field(default = 0.05, metadata = {\"help\": \"update matrics에서 dropout 적용 확률\"})\n",
    "    bias: str = field(default = \"none\", metadata = {\"help\": \"update matrix에 bias를 학습할 것인지 선택\"})\n",
    "    task_type: str = field(default = \"CAUSAL_LM\", metadata = {\"help\": \"학습할 모형이 무엇인지 지정\"})\n",
    "    target_modules: list[str] = field(default = None, metadata = {\"help\": \"학습에 반영할 모듈 설정\"})\n",
    "\n",
    "\n",
    "def timer(func):\n",
    "    \"\"\"\n",
    "    함수 실행 시간 출력\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import datetime\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "\n",
    "        sec = end - start\n",
    "        worktime = str(datetime.timedelta(seconds=sec)).split(\".\")[0]\n",
    "        print(f\"Working Time: {worktime}\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def seeding(seed):\n",
    "    \"\"\"\n",
    "    시드 설정으로 인해 성능이 저하될 수 있음. dataloader worker에도 시드 설정이 필요할 수 있음\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)                 ## cpu seed\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)        ## gpu seed\n",
    "        torch.cuda.manual_seed_all(seed)    ## multi-gpu seed\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True   ## nondeterministic algorithm을 사용하지 않도록 설정\n",
    "    torch.backends.cudnn.benchmark = False      ## cuDNN의 여러 convolution algorithm들을 실행하고, 벤치마킹하여 가장 빠른 알고리즘 사용: 안함.\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)    ## hash 알고리즘 관련\n",
    "    os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"   ## oneDNN 옵션 해제. 수치 연산 순서 고정 (성능 저하, 속도 저하)\n",
    "\n",
    "@timer\n",
    "def main(script_args, training_args, lora_kwargs):\n",
    "    ## loading dataset\n",
    "    train_ds = load_dataset(\"json\", data_files = os.path.join(script_args.dataset_path, \"sft_train_dataset.json\"), split = \"train\")\n",
    "    test_ds = load_dataset(\"json\", data_files = os.path.join(script_args.dataset_path, \"sft_test_dataset.json\"), split = \"train\")\n",
    "\n",
    "    print(f\"training dataset size: {train_ds.num_rows}\\ntest dataset size: {test_ds.num_rows}\")\n",
    "\n",
    "    ## 양자화 설정\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,                    ## 4비트 양자화\n",
    "        bnb_4bit_use_double_quant = True,       ## 추가 양자화로 성능 손실 없이 파라미터당 0.4bit 추가 절약\n",
    "        bnb_4bit_quant_type = \"nf4\",            ## 양자화 데이터 타입 지정: 4비트 기반 모델 훈련 시 사용\n",
    "        bnb_4bit_compute_dtype = torch.bfloat16 ## Llama-3.1-8B의 학습 자료형. 저장은 4비트지만, attention 연산은 해당 포맷으로 역양자화하여 처리\n",
    "    )\n",
    "\n",
    "    ## 모델 로드 및 설정\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        script_args.model_name,\n",
    "        device_map = \"cuda:0\",\n",
    "        use_cache = False,                          ## VRAM 캐시 미사용, 추론 속도 저하. gradienc_checkpointing과 동시 사용 불가\n",
    "        low_cpu_mem_usage = True,\n",
    "        attn_implementation = \"flash_attention_2\",  ## flash_attention 연산 사용. sdpa가 더 빠르고 효율적일 수도 있음.\n",
    "        quantization_config = bnb_config,\n",
    "        dtype = torch.bfloat16                      ## 가중치 로드 데이터 타입. Llama-3.1-8B의 자료형으로 설정\n",
    "    )\n",
    "\n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    peft_config = LoraConfig(**lora_kwargs)\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_ds,\n",
    "        eval_dataset = test_ds,\n",
    "        processing_class = tokenizer,\n",
    "        peft_config = peft_config\n",
    "    )\n",
    "\n",
    "    if training_args.assistant_only_loss:\n",
    "        print(\"======== Log a first sample from the processed training set ========\")\n",
    "        print(f\"masking area: {next(iter(trainer.train_dataset))[\"assistant_masks\"][:100]} ...\")\n",
    "\n",
    "    ## 학습이 중단된 경우 이어서 진행할 수 있도록 설정\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "\n",
    "    inference_callback = utils.SaveInferenceResultsCallback(trainer=trainer, test_dataset=test_ds, model_name=training_args.output_dir.split(\"/\")[-1])\n",
    "    trainer.add_callback(inference_callback)\n",
    "\n",
    "    trainer.train(resume_from_checkpoint = checkpoint)\n",
    "    trainer.save_model()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"wandb\", exist_ok = True)\n",
    "    # initial_folders = set(next(os.walk(\"wandb\"))[1])\n",
    "\n",
    "    parser = TrlParser((ScriptArguments, SFTConfig, LoraArguments))         ## 따로 저장된 파라미터 파싱\n",
    "    script_args, training_args, lora_args = parser.parse_args_and_config()\n",
    "\n",
    "    ## Lora Config에 유효한 입력값만 받을 수 있도록 커스터마이징. 원래 TrlParser에는 LoraConfig를 넣지 못함\n",
    "    valid_keys = LoraConfig.__init__.__code__.co_varnames\n",
    "    lora_kwargs = {\n",
    "        f.name: getattr(lora_args, f.name)\n",
    "        for f in fields(lora_args)\n",
    "        if f.name in valid_keys\n",
    "    }\n",
    "\n",
    "    # seeding(training_args.seed)\n",
    "\n",
    "    main(script_args, training_args, lora_kwargs)\n",
    "\n",
    "    print(\"========== 학습 종료 ==========\")\n",
    "\n",
    "    ## ========== 추론 파일 종합 ===========\n",
    "    utils.excel_integrate(training_args.output_dir.split(\"/\")[-1])\n",
    "\n",
    "    ## ========== wandb 업로드 ==========\n",
    "    # current_folders = set(next(os.walk(\"wandb\"))[1])\n",
    "    # new_folders = current_folders - initial_folders\n",
    "    # os.system(f\"wandb sync --include-offline ./wandb/{list(current_folders)[0]}\")\n",
    "    os.system(f\"wandb sync --include-offline wandb/latest-run\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
