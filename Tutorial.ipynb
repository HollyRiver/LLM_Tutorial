{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dadfc33",
   "metadata": {},
   "source": [
    "# HuggingFace LLM과 공개 데이터셋을 활용한 RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd56239",
   "metadata": {},
   "source": [
    "## **1. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d405c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/LLM/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## ============= Basic Modules =============\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from dataclasses import dataclass, field, fields    ## For TrlParser\n",
    "\n",
    "## ================ Utility ================\n",
    "import os\n",
    "import gc\n",
    "import argparse\n",
    "import wandb        ## External Training Log Analysis Tool\n",
    "\n",
    "## ========== Hugging Face Library ==========\n",
    "from huggingface_hub import login   ## Input Token\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import (\n",
    "    SFTTrainer, SFTConfig,  ## SFT\n",
    "    DPOConfig, DPOTrainer,  ## DPO\n",
    "    TrlParser               ## YAML Parameter Parsing\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer,    ## Vanilla Model Loading\n",
    "    HfArgumentParser, TrainingArguments,    ## Tuning Parameter Setting\n",
    "    BitsAndBytesConfig,                     ## Quantization\n",
    "    set_seed                                ## Seeding (not recommended)\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,                             ## For Low-Rank Adaption\n",
    "    PeftConfig, AutoPeftModelForCausalLM, PeftModel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78763284",
   "metadata": {},
   "source": [
    "## **2. UltraFeedback Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d02ed",
   "metadata": {},
   "source": [
    "* 고품질 AI 피드백 데이터셋. PPO에 활용되는 데이터셋을 이진 선호도 데이터셋으로 변환한 형태(RLAIF)\n",
    "* 특정 태스크에 목적이 있는 게 아닌, 다양한 주제에서의 채팅 기능 향상을 위한 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e42bc28",
   "metadata": {},
   "source": [
    "`-` 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd83df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 원시 데이터 로드\n",
    "ds = load_dataset(\"argilla/ultrafeedback-binarized-preferences-cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e6a414",
   "metadata": {},
   "source": [
    "`-` SFT / DPO 데이터셋 분리\n",
    "\n",
    "* 본래는 SFT를 위한 데이터셋과 DPO를 위한 데이터셋이 따로 준비되어 있어야 하지만, DPO 데이터셋만 존재하기 때문에 실습을 위해 둘을 분리하여 훈련을 진행합니다.\n",
    "* 실제 추천 프레임워크\n",
    "\n",
    "    1. 원하는 태스크의 라벨링된 데이터셋으로 SFT 수행\n",
    "    2. SFT를 완수한 모델을 이용하여 한 질문에 대하여 두 개 이상의 답변 생성\n",
    "    3. 생성된 답변을 조합하여 Chosen / Rejected 쌍으로 구성된 선호도 데이터셋 제작(HF or AIF)\n",
    "    > 해당 과정에서 가장 좋은 답변만을 Chosen으로 설정할 수 있음\n",
    "    >\n",
    "    > n개 답변을 생성했다면, 그중 한 개의 답변을 최선으로 설정한 뒤, 이를 Chosen으로 설정. 총 n-1개의 선호도 쌍을 생성 가능\n",
    "    >\n",
    "    > 일반적으로 순위를 매긴 뒤, 상위 몇 개의 답변만 Chosen으로 설정한 다음 조합하여 선호도 쌍을 생성\n",
    "    4. 선호도 데이터셋을 사용하여 SFT를 수행한 모델에 DPO를 이어서 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628a28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"main/data\", exist_ok = True)    ## 디렉토리 생성\n",
    "\n",
    "ds_split = ds[\"train\"].train_test_split(test_size = 0.5, seed = 42) ## SFT/DPO 데이터 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d7dc2",
   "metadata": {},
   "source": [
    "`-` 데이터셋 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e54d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format:   0%|          | 0/28 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 28/28 [00:00<00:00, 38.42ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 49.61ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 28/28 [00:01<00:00, 20.87ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 26.96ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10251164"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For SFT\n",
    "sft_ds = ds_split[\"train\"]\n",
    "sft_ds = sft_ds.rename_column(\"chosen\", \"messages\").remove_columns([col for col in sft_ds.column_names if col != \"chosen\"])\n",
    "## 시스템 프롬프트 추가\n",
    "# with open(\"./main/data/system_prompt.txt\", \"r\") as f:\n",
    "#     system_text = f.read()\n",
    "# \n",
    "# sft_ds.map(\n",
    "#     lambda sample: {\n",
    "#         \"messages\": [{\"role\": \"system\", \"content\": system_text}] + sample[\"messages\"]\n",
    "#     }\n",
    "# )\n",
    "sft_ds = sft_ds.train_test_split(test_size = 0.1, seed = 42)\n",
    "sft_ds[\"train\"].to_json(\"./main/data/sft_train_dataset.json\", orient = \"records\")\n",
    "sft_ds[\"test\"].to_json(\"./main/data/sft_test_dataset.json\", orient = \"records\")\n",
    "\n",
    "## For DPO: Implicit Prompt -> Explicit Prompt (Recommanded)\n",
    "dpo_ds = ds_split[\"test\"].map(\n",
    "    lambda sample: {\n",
    "        \"prompt\": [\n",
    "            # {\"role\": \"system\", \"content\": system_text}, ## 이 자리에 시스템 프롬프트를 입력할 수 있습니다.\n",
    "            {\"role\": \"user\", \"content\": sample[\"prompt\"]}\n",
    "        ],\n",
    "        \"chosen\": [content for content in sample[\"chosen\"] if content[\"role\"] == \"assistant\"],\n",
    "        \"rejected\": [content for content in sample[\"rejected\"] if content[\"role\"] == \"assistant\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "dpo_ds = dpo_ds.remove_columns([col for col in dpo_ds.column_names if col not in [\"prompt\", \"chosen\", \"rejected\"]]).train_test_split(test_size = 0.1, seed = 42)\n",
    "dpo_ds[\"train\"].to_json(\"./main/data/dpo_train_dataset.json\", orient = \"records\")\n",
    "dpo_ds[\"test\"].to_json(\"./main/data/dpo_test_dataset.json\", orient = \"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4111ba16",
   "metadata": {},
   "source": [
    "> 자세한 내용은 `main` 폴더의 `csv_to_json_dataset.py` 파일을 확인하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8baa6d",
   "metadata": {},
   "source": [
    "## **3. Model Loading**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d4f4f8",
   "metadata": {},
   "source": [
    "* 허깅페이스에 로그인하여 권한을 획득하고, [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) 모델을 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22adab4",
   "metadata": {},
   "source": [
    "`-` 허깅페이스에 로그인\n",
    "\n",
    "* Access Token을 발급받은 뒤, 모델 사용 권한을 요청하면 잠시 뒤 모델에 접근할 수 있습니다.\n",
    "* 허깅페이스 프로필 아이콘 클릭 > Access Tokens에서 발급 가능\n",
    "* [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) 모델 접근 권한 획득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124cbbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token = \"hf_...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a042bec7",
   "metadata": {},
   "source": [
    "> 해당 방식은 시연을 위한 방법이고, `login()`으로 작성하여 `token` 파라미터를 사용하지 않은 상태에서 터미널에서 토큰을 입력해 로그인하는 것이 안전합니다.\n",
    ">\n",
    "> 일반적으로는 터미널에서 `hf auth login` 명령어를 통해 미리 로그인해주는 것이 편합니다. 이러면 해당 로그인 코드는 필요하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a0d9a",
   "metadata": {},
   "source": [
    "`-` 모델 및 토크나이저 로드\n",
    "\n",
    "* 거의 대부분의 모델은 두 개의 객체를 함께 로드합니다.\n",
    "* 토크나이저(Tokenizer): 텍스트를 단어(실제론 이보다 작은 단위)로 쪼갠 다음(Tokenize), Token ID로 매핑\n",
    "* 모델(Model): 임베딩 후 생성\n",
    "\n",
    "    > 양자화 설정을 여기서 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f884a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "## 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    use_fast = True,            ## Rust로 구현된 Fast Tokenizer 사용 (Qwen, RoPE, ChatGLM 등의 특이한 구조에서는 호환 안됨)\n",
    "    trust_remote_code = True    ## 모델 코드 전체 다운로드 후 사용\n",
    ")\n",
    "\n",
    "## 양자화 설정: 모델의 가중치를 로드할 때 양자화하여 들여옵니다.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,                    ## 4비트로 양자화\n",
    "    bnb_4bit_use_double_quant = True,       ## 추가 양자화(스케일 파라미터 양자화) 활성화. 메모리 절약\n",
    "    bnb_4bit_quant_type = \"nf4\",            ## 양자화 데이터 타입 지정: 4비트 기반 모델 훈련 시 사용\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 ## 4비트로 로드하지만, attention 연산 시 해당 포맷으로 역양자화하여 처리 (라마 기본 자료형, 거의 대부분의 최신 LLM은 해당 포맷 사용)\n",
    ")\n",
    "\n",
    "## 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",         ## 사용 모델명\n",
    "    device_map = \"cuda:0\",                      ## GPU 사용\n",
    "    use_cache = False,                          ## KV 캐시 미사용(VRAM), 추론 속도 저하. gradienc_checkpointing과 동시 사용 불가\n",
    "    attn_implementation = \"flash_attention_2\",  ## flash_attention 연산 사용. sdpa가 더 빠르고 효율적일 수도 있음.\n",
    "    dtype = torch.bfloat16,                     ## 초기 가중치 로드 데이터 타입. Llama-3.1-8B의 자료형으로 설정\n",
    "    quantization_config = bnb_config            ## 양자화 설정 적용\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22dec26",
   "metadata": {},
   "source": [
    "> 처음 실행할 때 모델을 다운로드하기 때문에 적잖은 시간이 소요됩니다. 다음 실행할 때 부터는 빠르게 실행됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50f051",
   "metadata": {},
   "source": [
    "`-` 모델 및 토크나이저 설정\n",
    "\n",
    "* 토크나이저를 일부 커스터마이징합니다. Chat Template 설정이 주요 요소입니다.\n",
    "* Chat Template는 기본적인 템플릿이 따로 존재하지만, SFT에서 `assistant_only_loss`로 학습을 위해서는 추가적인 양식이 요구됩니다.\n",
    "\n",
    "    > `prompt-completion` 포맷의 데이터셋은 `completion_only_loss`를 활성화하는 것만으로도 간단하게 라벨 부분만으로 손실을 계산할 수 있습니다.\n",
    "    >\n",
    "    > 일부 모델의 토크나이저에는 `generation` 부분이 템플릿에 포함되어 있어 그대로 사용해도 될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daa5b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token       ## 패딩할 토큰 설정 (padding_free 설정 시 큰 의미는 없음)\n",
    "tokenizer.padding_side = \"left\"                 ## 디코더이므로 왼쪽을 패딩 (마지막 토큰을 보고 생성)\n",
    "\n",
    "## 데이터셋에 적합한 chat template 적용: {% generation %} 부분을 추가하여 assistant_only_loss 진행\n",
    "## 모든 텍스트로 손실을 계산하고자 한다면 tokenizer에 기본으로 할당된 chat template로 충분\n",
    "## jinja2 template engine 구문. 파이썬 문법과 거의 동일\n",
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"{{ bos_token }}\"\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + eos_token }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n'}}\"\n",
    "            \"{% generation %}\"\n",
    "            \"{{ message['content'] +  eos_token }}\"\n",
    "            \"{% endgeneration %}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{%- if add_generation_prompt %}\"\n",
    "    \"{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"\n",
    "    \"{%- endif %}\"\n",
    ")\n",
    "\n",
    "## 새로운 Chat Template 적용\n",
    "tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81491b53",
   "metadata": {},
   "source": [
    "> 핵심은 `'assistant'`부분에 `{% generation %} ~ {% endgeneration %}`을 추가하는 것입니다. 이러면 해당 텍스트는 마스킹에서 제외됩니다.\n",
    ">\n",
    "> ```Python\n",
    "> \"{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n'}}\"\n",
    "> \"{% generation %}\"\n",
    "> \"{{ message['content'] +  eos_token }}\"\n",
    "> \"{% endgeneration %}\"\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71cd13",
   "metadata": {},
   "source": [
    "## **4. Supervised Fine Tuning (SFT)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ec36f",
   "metadata": {},
   "source": [
    "* 기본 모델을 원하는 태스크에 특화되도록 1차적인 파인튜닝을 시행합니다. 단일 GPU 환경을 가장합니다. 분산 GPU 환경에서의 학습은 [FSDP](https://huggingface.co/docs/transformers/ko/fsdp)를 참고하세요.\n",
    "* `YAML` 파일로 하이퍼파라미터를 쉽게 관리할 수 있습니다.\n",
    "\n",
    "    > 단순 `argparse` 모듈로 파라미터 설정을 할 수도 있지만, 이 경우 `accelerate` 라이브러리를 통한 분산 GPU 환경에서의 학습에 호환되지 않으므로 확장성을 위해 이와 같은 방식을 택했습니다.\n",
    "\n",
    "`(선택사항)`\n",
    "\n",
    "* 학습 과정에서 계산된 손실, 평균 토큰 정확도 등의 로그를 wandb를 통해 간단히 시각화/분석할 수 있습니다.\n",
    "* wandb 플랫폼에 로그인이 필요합니다. https://wandb.ai/site"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5248ea75",
   "metadata": {},
   "source": [
    "`-` 사전 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7f5bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zombie Process 발생 방지\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"    ## 수동 업데이트: wandb sync --include-offline ./wandb/latest-run\n",
    "wandb.init(project = \"Test Project\")    ## wandb에서 만든 프로젝트 이름 명시\n",
    "\n",
    "## 파이토치 백엔드에서 fp32대신 tf32 자료형을 사용하여 처리량 개선 (약간의 정확도 감소)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8c57d",
   "metadata": {},
   "source": [
    "`-` 유틸리티 구성\n",
    "\n",
    "* `@dataclass` 데코레이터를 활용하여 `TrlParser`에 들어갈 파라미터를 지정해줍니다.\n",
    "    \n",
    "    > 현재 SFT의 설정을 조정하는 `SFTConfig` 객체는 `TrlParser`에 완벽히 호환되나, 그 외의 config 객체들은 호환되지 않습니다.\n",
    "    >\n",
    "    > 따라서 `ScriptArguments`와 `LoraArguments`라는 클래스를 만들어 튜닝 파라미터를 간단히 수용하고 관리할 수 있게 만들었습니다.\n",
    "\n",
    "* 총 실행 시간을 계산하는 함수 `timer`와, 재현성을 위한 `seeding` 함수를 정의합니다.\n",
    "\n",
    "    > 해당 함수들은 선택사항입니다.\n",
    "    >\n",
    "    > 시드를 설정할 경우 비결정적 알고리즘을 사용할 수 없게됨에 따라 성능이 일부 감소할 수 있어 사용을 권장하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aac8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TrlParser에 들어갈 class들을 커스터마이징: 하이퍼파라미터 저장\n",
    "@dataclass  ## 데이터 보관 클래스를 간단하게 구축 가능: __init__, __repr__, __eq()__등의 메소드 자동 생성\n",
    "class ScriptArguments:\n",
    "    dataset_path: str = field(default = None, metadata = {\"help\": \"dataset directory\"})\n",
    "    model_name: str = field(default = None, metadata = {\"help\": \"사용할 모델 ID\"})\n",
    "\n",
    "@dataclass\n",
    "class LoraArguments:\n",
    "    r: int = field(default = 64, metadata = {\"help\": \"update matrix의 rank. 작을수록 많이 압축하여 품질 저하됨, 메모리 많이 할당됨\"})\n",
    "    lora_alpha: int = field(default = 32, metadata = {\"help\": \"∆Weight scaling factor. lora_alpha / r로 스케일링되며, 학습률 조정. 보통 1/2 수준으로 설정\"})\n",
    "    lora_dropout: float = field(default = 0.05, metadata = {\"help\": \"update matrics에서 dropout 적용 확률\"})\n",
    "    bias: str = field(default = \"none\", metadata = {\"help\": \"update matrix에 bias를 학습할 것인지 선택\"})\n",
    "    task_type: str = field(default = \"CAUSAL_LM\", metadata = {\"help\": \"학습할 모형이 무엇인지 지정\"})\n",
    "    target_modules: list[str] = field(default = None, metadata = {\"help\": \"학습에 반영할 모듈 설정\"})\n",
    "\n",
    "\n",
    "def timer(func):\n",
    "    \"\"\"\n",
    "    함수 실행 시간 출력\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import datetime\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "\n",
    "        sec = end - start\n",
    "        worktime = str(datetime.timedelta(seconds=sec)).split(\".\")[0]\n",
    "        print(f\"Working Time: {worktime}\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def seeding(seed):\n",
    "    \"\"\"\n",
    "    시드 설정으로 인해 성능이 저하될 수 있음. dataloader worker에도 시드 설정이 필요할 수 있음\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)                 ## cpu seed\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)        ## gpu seed\n",
    "        torch.cuda.manual_seed_all(seed)    ## multi-gpu seed\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True   ## nondeterministic algorithm을 사용하지 않도록 설정\n",
    "    torch.backends.cudnn.benchmark = False      ## cuDNN의 여러 convolution algorithm들을 실행하고, 벤치마킹하여 가장 빠른 알고리즘 사용: 안함.\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)    ## hash 알고리즘 관련\n",
    "    os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"   ## oneDNN 옵션 해제. 수치 연산 순서 고정 (성능 저하, 속도 저하)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38df3b7",
   "metadata": {},
   "source": [
    "`-` 메인 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6eec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer  ## 총 소요 시간 측정 및 출력\n",
    "def main(script_args, training_args, lora_kwargs):\n",
    "    ## loading dataset\n",
    "    train_ds = load_dataset(\"json\", data_files = os.path.join(script_args.dataset_path, \"sft_train_dataset.json\"), split = \"train\")\n",
    "    test_ds = load_dataset(\"json\", data_files = os.path.join(script_args.dataset_path, \"sft_test_dataset.json\"), split = \"train\")\n",
    "\n",
    "    ## 데이터셋 사이즈 출력\n",
    "    print(f\"training dataset size: {train_ds.num_rows}\\ntest dataset size: {test_ds.num_rows}\")\n",
    "\n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    peft_config = LoraConfig(**lora_kwargs)\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_ds,\n",
    "        eval_dataset = test_ds,\n",
    "        processing_class = tokenizer,\n",
    "        peft_config = peft_config\n",
    "    )\n",
    "\n",
    "    if training_args.assistant_only_loss:\n",
    "        print(\"======== Log a first sample from the processed training set ========\")\n",
    "        print(f\"masking area: {next(iter(trainer.train_dataset))[\"assistant_masks\"][:100]} ...\")\n",
    "\n",
    "    ## 학습이 중단된 경우 이어서 진행할 수 있도록 설정\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "\n",
    "    trainer.train(resume_from_checkpoint = checkpoint)\n",
    "    trainer.save_model()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"wandb\", exist_ok = True)\n",
    "    # initial_folders = set(next(os.walk(\"wandb\"))[1])\n",
    "\n",
    "    ## --config config/SFT_config.yaml 대신 임시로...\n",
    "    ## ======================================================\n",
    "    ## 파라미터 파싱은 파이썬 파일로 된 코드를 보시는 편이 더 정확합니다.\n",
    "    config_path = \"./main/config/SFT_config.yaml\"\n",
    "    parser = TrlParser((ScriptArguments, SFTConfig, LoraArguments))         ## 따로 저장된 파라미터 파싱\n",
    "    script_args, training_args, lora_args = parser.parse_args_and_config(\n",
    "        args=[\"--config\", config_path]\n",
    "    )\n",
    "\n",
    "    ## Lora Config에 유효한 입력값만 받을 수 있도록 커스터마이징. 원래 TrlParser에는 LoraConfig를 넣지 못함\n",
    "    valid_keys = LoraConfig.__init__.__code__.co_varnames\n",
    "    lora_kwargs = {\n",
    "        f.name: getattr(lora_args, f.name)\n",
    "        for f in fields(lora_args)\n",
    "        if f.name in valid_keys\n",
    "    }\n",
    "\n",
    "    # seeding(training_args.seed)\n",
    "\n",
    "    main(script_args, training_args, lora_kwargs)\n",
    "\n",
    "    print(\"========== 학습 종료 ==========\")\n",
    "\n",
    "    ## ========== wandb 업로드 ==========\n",
    "    os.system(f\"wandb sync --include-offline wandb/latest-run\") ## 가장 마지막에 진행한 튜닝 작업의 로그 제출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbeaf5d",
   "metadata": {},
   "source": [
    "> 원래는 callback을 위한 클래스 커스터마이징까지 넣으려고 했는데, 내용도 길어지고 필요 없을 수도 있는 부분이라 참고용 링크만 제공해드립니다.\n",
    "\n",
    "* [Callback (HuggingFace)](https://huggingface.co/docs/transformers/v5.0.0rc1/en/main_classes/callback#transformers.TrainerCallback)\n",
    "* [TrainerCallback (Github)](https://github.com/huggingface/transformers/blob/v5.0.0rc1/src/transformers/trainer_callback.py#L295)\n",
    "\n",
    "    `transformers.TrainerCallback` 모듈을 상속하여 클래스 구성한 뒤, 원하는 이름의 메서드를 overriding하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7713d4eb",
   "metadata": {},
   "source": [
    "## **5. (1차) 튜닝 모델 테스트**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d155de56",
   "metadata": {},
   "source": [
    "* 소수의 샘플만 테스트하고 싶다면, HuggingFace 라이브러리만 사용해도 충분합니다. 에폭별 Callback 등에 사용하는 경우 이쪽이 vLLM보다 빠르고 효율적입니다.\n",
    "* 단순히 validation loss가 최소이거나 마지막 체크포인트를 불러올 수도 있으나, 여러 체크포인트에서 불러온 모델들을 테스트하고 그 중 보았을 때 가장 괜찮은 결과를 생성한 버전을 선택하는 것도 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ba8ad6",
   "metadata": {},
   "source": [
    "`-` VRAM 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22ae341",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c895929",
   "metadata": {},
   "source": [
    "`-` 테스트에 사용될 데이터 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f64ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 무작위 1개 텍스트 추출\n",
    "test_dataset = load_dataset(\"json\", data_files = os.path.join(\"\", \"data/sft_test_dataset.json\"), split = \"train\")\n",
    "random_idx = random.randint(0, len(test_dataset))\n",
    "\n",
    "## 출력하여 확인\n",
    "for k, v in test_dataset[random_idx].items():\n",
    "    print(f\"{k}: {v[0][\"content\"]}\\n\\n\")\n",
    "\n",
    "messages = test_dataset[random_idx][\"prompt\"]\n",
    "chosen = test_dataset[random_idx][\"chosen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83980d93",
   "metadata": {},
   "source": [
    "`-` 원시 모델 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 로드 및 초기 설정\n",
    "origin_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "origin_model = AutoModelForCausalLM.from_pretrained(origin_model_name, use_cache = False, device_map = \"cuda:0\", dtype = torch.bfloat16)\n",
    "origin_tokenizer = AutoTokenizer.from_pretrained(origin_model_name, use_fast = True)\n",
    "origin_tokenizer.pad_token = origin_tokenizer.eos_token\n",
    "origin_tokenizer.pad_token_id = origin_tokenizer.eos_token_id\n",
    "origin_tokenizer.padding_side = \"left\"\n",
    "\n",
    "terminators = [origin_tokenizer.eos_token_id]\n",
    "\n",
    "input_ids = origin_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,   ## 생성 시에 맨 뒤 generation 시작하라는 프롬프트 삽입\n",
    "    return_tensors = \"pt\").to(origin_model.device)\n",
    "\n",
    "## 생성\n",
    "outputs = origin_model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens = 512,\n",
    "    eos_token_id = terminators,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.95\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(f\"prompt:\\n{messages[0][\"content\"]}\\n\")\n",
    "print(f\"chosen:\\n{chosen[0][\"content\"]}\\n\")\n",
    "print(f\"generate:\\n{origin_tokenizer.decode(response, skip_special_tokens = True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a5f7a",
   "metadata": {},
   "source": [
    "`-` 베이스 모델과 어뎁터 로드 및 비교\n",
    "\n",
    "* 베이스 모델을 4비트로 양자화한 뒤, 어뎁터를 부착"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "adapter_name = \"../adapter/SFT_Adapter\" ## 마지막 체크포인트 로드 (상황에 따라 조정)\n",
    "\n",
    "## 양자화 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    quantization_config = bnb_config,\n",
    "    attn_implementation = \"eager\",\n",
    "    use_cache = True,\n",
    "    dtype = torch.bfloat16,\n",
    "    device_map = \"cuda:0\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, adapter_name)  ## 어뎁터 부착\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_name, use_fast = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "terminators = [tokenizer.eos_token_id]\n",
    "\n",
    "## 생성\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,   ## 생성 시에 맨 뒤 generation 시작하라는 프롬프트 삽입\n",
    "    return_tensors = \"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens = 512,       ## prompt 제외 출력 토큰 수. max_length는 prompt 포함\n",
    "    eos_token_id = terminators,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.95\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(f\"prompt:\\n{messages[0][\"content\"]}\\n\")\n",
    "print(f\"chosen:\\n{chosen[0][\"content\"]}\\n\")\n",
    "print(f\"generate:\\n{tokenizer.decode(response, skip_special_tokens = True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc69afd6",
   "metadata": {},
   "source": [
    "## **6. Direct Preference Optimization (DPO)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f80acf",
   "metadata": {},
   "source": [
    "* 대부분의 설정은 SFT에서와 동일합니다. 하지만 QLoRA 튜닝 환경에서는 중요한 설정이 하나 있습니다.\n",
    "* 일반 LoRA는 모델과 어뎁터의 가중치를 단순히 더하는 방식으로 모델을 병합(Merge)할 수 있습니다. 하지만, 양자화를 하게 되면 만약 병합을 진행하였을 시 어뎁터의 훈련된 가중치 정보(16비트)까지 4비트로 뭉개버린 다음, 이것에 어뎁터를 부착하여 DPO 학습을 진행하게 됩니다.\n",
    "\n",
    "    > 따라서 어뎁터를 새로 부착하지 않고, 현재 선택된 최고의 모델(어뎁터)를 이어서 학습하는 방식을 택합니다.\n",
    "    >\n",
    "    > [DPO에서 PEFT 사용 시 가능한 방법(HuggingFace)](https://huggingface.co/docs/trl/dpo_trainer#reference-model-considerations-with-peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84ba35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    dataset_path: str = field(default = None, metadata = {\"help\": \"dataset directory\"})\n",
    "    model_name: str = field(default = None, metadata = {\"help\": \"사용할 모델 ID\"})\n",
    "    adapter_name: str = field(default = None, metadata = {\"help\": \"SFT 완료된 어뎁터\"})\n",
    "\n",
    "@timer\n",
    "def main(script_args, training_args):\n",
    "    ## loading dataset\n",
    "    train_ds = load_dataset(\"json\", data_files = os.path.join(script_args.dataset_path, \"dpo_train_dataset.json\"), split = \"train\")\n",
    "    test_ds = load_dataset(\"json\", data_files = os.path.join(script_args.dataset_path, \"dpo_test_dataset.json\"), split = \"train\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        script_args.model_name,\n",
    "        use_fast = True,\n",
    "        trust_remote_code = True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    LLAMA_3_CHAT_TEMPLATE = (\n",
    "        \"{{ bos_token }}\"\n",
    "        \"{% for message in messages %}\"\n",
    "            \"{% if message['role'] == 'system' %}\"\n",
    "                \"{{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + eos_token }}\"\n",
    "            \"{% elif message['role'] == 'user' %}\"\n",
    "                \"{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] +  eos_token }}\"\n",
    "            \"{% elif message['role'] == 'assistant' %}\"\n",
    "                \"{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n'}}\"\n",
    "                \"{{ message['content'] +  eos_token }}\"\n",
    "            \"{% endif %}\"\n",
    "        \"{% endfor %}\"\n",
    "        \"{%- if add_generation_prompt %}\"\n",
    "        \"{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"\n",
    "        \"{%- endif %}\"\n",
    "    )\n",
    "\n",
    "    tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n",
    "\n",
    "    ## 양자화 설정\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_use_double_quant = True,\n",
    "        bnb_4bit_quant_type = \"nf4\",\n",
    "        bnb_4bit_compute_dtype = torch.bfloat16\n",
    "    )\n",
    "\n",
    "    ## 모델 로드 및 설정\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        script_args.model_name,\n",
    "        device_map = \"cuda:0\",\n",
    "        use_cache = False,\n",
    "        low_cpu_mem_usage = True,\n",
    "        attn_implementation = \"flash_attention_2\",\n",
    "        trust_remote_code = True,\n",
    "        quantization_config = bnb_config,\n",
    "        dtype = torch.bfloat16\n",
    "    )\n",
    "\n",
    "    ## 어뎁터 부착\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        script_args.adapter_name,\n",
    "        is_trainable=True,\n",
    "        adapter_name=\"policy\",\n",
    "    )\n",
    "\n",
    "    ## 어뎁터를 두 번 로드하여 하나는 Policy, 하나는 Reference로 지정\n",
    "    model.load_adapter(script_args.adapter_name, adapter_name = \"reference\")\n",
    "    model.set_adapter(\"policy\")\n",
    "\n",
    "    training_args.model_adapter_name = \"policy\"\n",
    "    training_args.ref_adapter_name = \"reference\"\n",
    "\n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    trainer = DPOTrainer(\n",
    "        model,\n",
    "        args = training_args,\n",
    "        train_dataset= train_ds,\n",
    "        eval_dataset = test_ds,\n",
    "        processing_class = tokenizer\n",
    "    )\n",
    "\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "\n",
    "    trainer.train(resume_from_checkpoint = checkpoint)\n",
    "    trainer.save_model()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = TrlParser((ScriptArguments, SFTConfig, LoraArguments))         ## 따로 저장된 파라미터 파싱\n",
    "    script_args, training_args, lora_args = parser.parse_args_and_config(\n",
    "        args=[\"--config\", config_path]\n",
    "    )\n",
    "\n",
    "    ## Lora Config에 유효한 입력값만 받을 수 있도록 커스터마이징. 원래 TrlParser에는 LoraConfig를 넣지 못함\n",
    "    valid_keys = LoraConfig.__init__.__code__.co_varnames\n",
    "    lora_kwargs = {\n",
    "        f.name: getattr(lora_args, f.name)\n",
    "        for f in fields(lora_args)\n",
    "        if f.name in valid_keys\n",
    "    }\n",
    "\n",
    "    # seeding(training_args.seed)\n",
    "\n",
    "    main(script_args, training_args, lora_kwargs)\n",
    "\n",
    "    print(\"========== 학습 종료 ==========\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config_path = \"./main/config/DPO_config.yaml\"\n",
    "    parser = TrlParser((ScriptArguments, DPOConfig))\n",
    "    script_args, training_args, lora_args = parser.parse_args_and_config(\n",
    "        args=[\"--config\", config_path]\n",
    "    )\n",
    "\n",
    "    main(script_args, training_args)\n",
    "\n",
    "    print(\"========== 학습 종료 ==========\")\n",
    "\n",
    "    ## ========== wandb 업로드 ==========\n",
    "    os.system(f\"wandb sync --include-offline wandb/latest-run\") ## 가장 마지막에 진행한 튜닝 작업의 로그 제출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a77ed",
   "metadata": {},
   "source": [
    "## **7. (2차) 최종 튜닝 모델 테스트**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35e8a9e",
   "metadata": {},
   "source": [
    "* HuggingFace 라이브러리를 통해 한 개의 텍스트에 대한 생성을 비교 (원본 / SFT / SFT + DPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89564810",
   "metadata": {},
   "source": [
    "`-` VRAM 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25717c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d1453",
   "metadata": {},
   "source": [
    "`-` 테스트에 활용될 데이터 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97aaef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 무작위 1개 텍스트 추출\n",
    "test_dataset = load_dataset(\"json\", data_files = os.path.join(\"\", \"data/sft_test_dataset.json\"), split = \"train\")\n",
    "random_idx = random.randint(0, len(test_dataset))\n",
    "\n",
    "## 출력하여 확인\n",
    "for k, v in test_dataset[random_idx].items():\n",
    "    print(f\"{k}: {v[0][\"content\"]}\\n\\n\")\n",
    "\n",
    "messages = test_dataset[random_idx][\"prompt\"]\n",
    "chosen = test_dataset[random_idx][\"chosen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f10337",
   "metadata": {},
   "source": [
    "`-` 원시 모델 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe91d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 로드 및 초기 설정\n",
    "origin_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "origin_model = AutoModelForCausalLM.from_pretrained(origin_model_name, use_cache = False, device_map = \"cuda:0\", dtype = torch.bfloat16)\n",
    "origin_tokenizer = AutoTokenizer.from_pretrained(origin_model_name, use_fast = True)\n",
    "origin_tokenizer.pad_token = origin_tokenizer.eos_token\n",
    "origin_tokenizer.pad_token_id = origin_tokenizer.eos_token_id\n",
    "origin_tokenizer.padding_side = \"left\"\n",
    "\n",
    "terminators = [origin_tokenizer.eos_token_id]\n",
    "\n",
    "input_ids = origin_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,   ## 생성 시에 맨 뒤 generation 시작하라는 프롬프트 삽입\n",
    "    return_tensors = \"pt\").to(origin_model.device)\n",
    "\n",
    "## 생성\n",
    "outputs = origin_model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens = 512,\n",
    "    eos_token_id = terminators,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.95\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(f\"prompt:\\n{messages[0][\"content\"]}\\n\")\n",
    "print(f\"chosen:\\n{chosen[0][\"content\"]}\\n\")\n",
    "print(f\"generate:\\n{origin_tokenizer.decode(response, skip_special_tokens = True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a40d91",
   "metadata": {},
   "source": [
    "`-` 베이스 모델과 SFT 어뎁터 로드 및 비교\n",
    "\n",
    "* 베이스 모델을 4비트로 양자화한 뒤, SFT 어뎁터를 부착"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "adapter_name = \"../adapter/SFT_Adapter\" ## 마지막 체크포인트 로드 (상황에 따라 조정)\n",
    "\n",
    "## 양자화 모델 로드\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    quantization_config = bnb_config,\n",
    "    attn_implementation = \"eager\",\n",
    "    use_cache = True,\n",
    "    dtype = torch.bfloat16,\n",
    "    device_map = \"cuda:0\"\n",
    ")\n",
    "\n",
    "sft_model = PeftModel.from_pretrained(sft_model, adapter_name)  ## 어뎁터 부착\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_name, use_fast = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "terminators = [tokenizer.eos_token_id]\n",
    "\n",
    "## 생성\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,   ## 생성 시에 맨 뒤 generation 시작하라는 프롬프트 삽입\n",
    "    return_tensors = \"pt\").to(sft_model.device)\n",
    "\n",
    "outputs = sft_model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens = 512,       ## prompt 제외 출력 토큰 수. max_length는 prompt 포함\n",
    "    eos_token_id = terminators,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.95\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(f\"prompt:\\n{messages[0][\"content\"]}\\n\")\n",
    "print(f\"chosen:\\n{chosen[0][\"content\"]}\\n\")\n",
    "print(f\"generate:\\n{tokenizer.decode(response, skip_special_tokens = True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d168a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sft_model\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c337505",
   "metadata": {},
   "source": [
    "`-` 베이스 모델과 DPO 어뎁터 로드 및 비교\n",
    "\n",
    "* 베이스 모델을 4비트로 양자화한 뒤, DPO 어뎁터를 부착"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e19d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "adapter_name = \"../adapter/DPO_Adapter/policy\" ## 마지막 체크포인트 로드 (wandb와 추론 결과를 보고 상황에 따라 조정)\n",
    "\n",
    "## 양자화 모델 로드\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    quantization_config = bnb_config,\n",
    "    attn_implementation = \"eager\",\n",
    "    use_cache = True,\n",
    "    dtype = torch.bfloat16,\n",
    "    device_map = \"cuda:0\"\n",
    ")\n",
    "\n",
    "dpo_model = PeftModel.from_pretrained(dpo_model, adapter_name)  ## 어뎁터 부착\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_name, use_fast = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "terminators = [tokenizer.eos_token_id]\n",
    "\n",
    "## 생성\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,   ## 생성 시에 맨 뒤 generation 시작하라는 프롬프트 삽입\n",
    "    return_tensors = \"pt\").to(dpo_model.device)\n",
    "\n",
    "outputs = dpo_model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens = 512,       ## prompt 제외 출력 토큰 수. max_length는 prompt 포함\n",
    "    eos_token_id = terminators,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.95\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(f\"prompt:\\n{messages[0][\"content\"]}\\n\")\n",
    "print(f\"chosen:\\n{chosen[0][\"content\"]}\\n\")\n",
    "print(f\"generate:\\n{tokenizer.decode(response, skip_special_tokens = True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49450caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dpo_model\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64700c89",
   "metadata": {},
   "source": [
    "## **8. vLLM Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189a7cd",
   "metadata": {},
   "source": [
    "* 모델의 빌드 및 최종 추론은 vLLM으로 수행\n",
    "* 대규모 데이터셋에서 HuggingFace 라이브러리보다 무조건 훨씬 빠름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc680e0d",
   "metadata": {},
   "source": [
    "`-` 4비트 양자화 base model 저장\n",
    "\n",
    "* vLLM은 기본적으로 서빙에 최적화된 툴이기 때문에, 한 개의 모델을 사용하는 것을 권장합니다.\n",
    "* 하지만 HuggingFace 버그로 인해 양자화된 모델과 어뎁터를 병합할 경우, 모델이 깨지는 (추론 결과가 완전히 다르게 나옴) 현상이 발생합니다. 아직 명쾌한 해결 방안은 찾지 못했습니다.\n",
    "\n",
    "    > 해당 포스트에서는 양자화 모델과 어뎁터의 병합이 불가능하다고 말합니다. https://medium.com/@bnjmn_marie/dont-merge-your-lora-adapter-into-a-4-bit-llm-65b6da287997\n",
    "    >\n",
    "    > 여러 방법을 시도해봤지만, 병합의 품질이 저조한 것으로 볼 때 nf4 포맷 자체의 특수성으로 인해 생기는 문제가 아닐까 생각됩니다. (bitsandbytes와 transformers LoRA 사이에서 발생하는 문제?)\n",
    "\n",
    "* 따라서 vLLM으로 추론 시에도 모델과 어뎁터를 따로 로드하여 생성을 진행합니다.\n",
    "* 추론에서의 양자와 자료형은 보통 훈련과 다른 포맷을 사용하기 때문에 vLLM에서 이를 직접적으로 설정하는 것은 어렵습니다. (AutoAWQ, GGUF 등의 포맷은 단순 파라미터 설정으로 명시할 수 있음)\n",
    "* 대신 vLLM은 모델 자체의 config 파일을 읽어 자료형을 가져올 수 있습니다.\n",
    "\n",
    "    1. 원본 라마 모델(16비트)을 vLLM에서 nf4로 양자화하는 것은 불가능\n",
    "    2. 이미 nf4로 양자화된 모델을 vLLM에서 로드하는 것은 가능\n",
    "    3. nf4 양자화 모델과 bf16 어뎁터를 결합하여 최종 추론 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b958567",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "save_directory = \"main/base_model/Llama-3.1-8B-Instruct-nf4\"\n",
    "adapter_name = \"main/adapter/SFT_Adapter\"   ## 토크나이저는 모든 과정에서 공유하므로 먼저 저장된 것을 사용합니다.\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    quantization_config = bnb_config,\n",
    "    use_cache = True,\n",
    "    dtype = torch.bfloat16,\n",
    "    device_map = \"cuda:0\"\n",
    ")\n",
    "\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_name)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e0b139",
   "metadata": {},
   "source": [
    "> 설명의 편의상 가장 마지막에 모델을 저장했지만, 해당 과정은 토크나이저 설정을 수행하신 다음 진행하셔도 좋습니다. Base Model과 Tokenizer 설정은 바뀌지 않으니까요. 물론 vLLM에 먹이기 전에만 하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55f7a90",
   "metadata": {},
   "source": [
    "`-` vLLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6707b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "## 오류 발생 시 아래 코드의 주석을 해제해볼 것\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ[\"VLLM_USE_V1\"] = \"0\" \n",
    "# os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "# os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "\n",
    "## ============== vLLM Inference ==============\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest   ## LoRA attach\n",
    "\n",
    "\n",
    "## apply chat template\n",
    "def template_dataset(example):\n",
    "    return {\"prompt\": tokenizer.apply_chat_template(example[\"messages\"], tokenize = False, add_generation_prompt = True)}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ## 입력 인자는 터미널에서 구동 시 argparse로 처리됩니다. main 폴더의 파일들을 참고해주세요.\n",
    "    base_model_path = \"base_model/Llama-3.1-8B-Instruct-nf4\"    ## 양자화하여 저장한 모델의 위치 (직접 저장해야 합니다.)\n",
    "    adapter_path = \"adapter/DPO_Adapter\"                        ## 최종 어뎁터 위치 (policy와 reference 폴더가 포함되어야 합니다.)\n",
    "    dataset_path = \"data/inference_data.json\"                   ## 생성에 사용할 파일의 위치 (json type). messages 열에 프롬프트가 전부 정리되어 있어야 합니다.\n",
    "    output_dir = \"data/inference_result.csv\"                    ## 생성 결과 저장 위치\n",
    "    gpu_memory_util = 0.5                                       ## GPU VRAM 최대 사용량 (점유율). 1.0으로 설정하면 불안정할 수 있으니 0.9나 0.95정도로 설정하면 충분합니다.\n",
    "    sampling = True                                             ## 샘플링 여부\n",
    "    repetition_penalty = 1.0                                    ## 반복적으로 같은 토큰을 생성함에 따른 패널티 부여. 1.0 이상으로 설정 시 적용됨\n",
    "\n",
    "    llm = LLM(\n",
    "        model= base_model_path,\n",
    "        dtype = torch.bfloat16,\n",
    "        trust_remote_code = True,\n",
    "        max_model_len = 2048,   ## Test dataset에서의 max token length보다 높은 값으로 설정\n",
    "        gpu_memory_utilization = gpu_memory_util,\n",
    "        enable_lora = True,\n",
    "        max_lora_rank = 64\n",
    "    )\n",
    "\n",
    "    ## 샘플링을 수행하는 경우: temperature / top_p를 다르게 설정할 수 있음\n",
    "    if sampling:\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature = 0.4,\n",
    "            top_p = 0.9,\n",
    "            max_tokens = 512,   ## 생성 텍스트의 토큰 길이\n",
    "            repetition_penalty = repetition_penalty\n",
    "        )\n",
    "\n",
    "    ## 샘플링을 하지 않는 경우: temperature를 0으로 설정하면 Greedy Search를 수행함\n",
    "    else:\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature = 0.0,\n",
    "            max_tokens = 512,   ## 생성 텍스트의 토큰 길이\n",
    "            repetition_penalty = repetition_penalty\n",
    "        )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast = True)\n",
    "    inference_data = load_dataset(\"json\", data_files = dataset_path, split = \"train\")\n",
    "    inference_data = inference_data.map(template_dataset, remove_columns = [\"messages\"])\n",
    "    prompts = inference_data[\"prompt\"]\n",
    "\n",
    "    outputs = llm.generate(\n",
    "        prompts, \n",
    "        sampling_params,\n",
    "        lora_request = LoRARequest(\"adapter\", 1, os.path.join(adapter_path, \"policy\"))\n",
    "    )\n",
    "\n",
    "    data = []\n",
    "    idx = inference_data[\"subject_id\"]\n",
    "\n",
    "    for idx, output in zip(idx, outputs):\n",
    "        row = {\n",
    "            \"subject_id\": idx,\n",
    "            \"generated_text\": output.outputs[0].text.strip()\n",
    "        }\n",
    "        data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_dir, index=False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
